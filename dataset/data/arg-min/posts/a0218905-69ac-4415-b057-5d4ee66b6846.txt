<article class="post-content">
<p>Would you say deep learning is mature enough to be taught in high schools?</p>
<p>Here’s why I ask. Some time ago, I received an email from a product manager at a
very large company. I love sharing personal emails publicly, so I’ll post it
here:</p>
<p class="center"><img alt="BUT HER EMAILS" src="/assets/optics/email.png" width="300px"/></p>
<p>I sat on this email for days. I couldn’t come up with a constructive answer.</p>
<p>If anything, I wanted to reply that maybe her engineers <em>should</em> be scared.</p>
<p class="center"><img alt="Pipelines. The source of all prosperity and misery" src="/assets/optics/vgg.png"/></p>
<p>Imagine you’re an engineer, you’re given this net, and you’re asked to make it
work better on a dataset. You might presume each of these layers is there for
a reason. But as a field, we don’t yet have a common language to express these
reasons. The way we teach deep learning is very different from the
way we teach other technical disciplines.</p>
<p>A few years ago, I got into optics. In optics, you also build stacks of
components that process inputs. Here’s a camera lens.</p>
<p class="center"><img alt="My eyes hurt" src="/assets/optics/lens.png" width="460px"/></p>
<p>To design something like this, you start with basic stackup, usually named after
the famous person who invented it, you simulate it to determine how it
falls short of your requirements, then insert additional components to correct the
shortcomings.</p>
<p>You then run the system through a numerical optimizer to tune its parameters, like
the shape of the surfaces, their locations, and their inclinations, to maximize some
design objective. Then you simulate, again, modify, optimize, and repeat.</p>
<p>Just like with a deep net.</p>
<p>Each of the 36 elements in the stackup is deliberately inserted to correct an
aberration of some kind. This requires a very clear mental model for what each
component does to the light that passes through it. That mental model is usually
in terms of an action, like refraction, reflection, diffraction,
dispersion, or wavefront correction.</p>
<p class="center"><img alt="refraction" height="90px" src="/assets/optics/refraction.gif" width="90px"/>
<img alt="reflection" height="90px" src="/assets/optics/reflection.jpg" width="90px"/>
<img alt="diffraction" height="90px" src="/assets/optics/diffraction.jpg" width="90px"/>
<img alt="dispersion" height="90px" src="/assets/optics/dispersion.jpg" width="110px"/>
<img alt="wavefront correction" height="90px" src="/assets/optics/wavefront-correction.png" width="110px"/></p>
<p>People don’t fear this design process. Each year, the US graduates hundreds of
optical engineers who go on to design lenses that work. And they’re not scared
of their job.</p>
<p>That’s not because optics is easy. It’s because the mental models
in optics are organized well.</p>
<p>Modern optics is taught in layers of abstraction.</p>
<p class="center"><img alt="Absractions all the way down" src="/assets/optics/abstractions.png" width="250px"/></p>
<p>At the simplest layer, at the top, is ray optics. Ray optics is a simplification
of wave optics, where the rays represent the normal vectors of the wavefronts in wave
optics. Wave optics is an approximate solution to Maxwell’s equations. And Maxwell’s
equations can be derived from quantum physics, which I don’t actually
understand.</p>
<p>Each layer is derived from the layer below it, by making simplifying
assumptions.  As such, each layer can explain a more sophisticated set of phenomena
than the layer above it.</p>
<p class="center"><img alt="Alchemy up top, science at the bottom" src="/assets/optics/optics-scorecard.png" width="650px"/></p>
<p>I spend most of my time designing systems at the top four levels of abstraction.</p>
<p>This is how we teach optics today.  But these theories weren’t always organized
in a stack like this. Until a hundred years ago, some of these theories co-existed
in a conflicting state.  Practitioners relied on folkloric theories of light.</p>
<p class="center"><img alt="And yet it magnifies" src="/assets/optics/galileo.jpg" width="250px"/></p>
<p>That didn’t stop Galileo from building a pretty good telescope nearly a hundred
years before Newton formalized ray optics. Galileo had a good enough mental model
of light to build a telescope that could magnify 10x. But his understanding
wasn’t good enough to correct chromatic aberrations, or to get a wide field of view.</p>
<p>Before these theories of light were unified in stack of abstractions, each theory
had to start with a fundamental concept of light from the ground up. This involves making up
a fresh set of unrealistic assumptions. Newton’s ray optics modeled light as a spray
of particles that could be attracted to, or repelled from, solid matter.  Huygens modeled light
as a longitudinal pressure wave through a mystical medium called “ether”. He
modeled light like sound.  Maxwell also assumed light propagates through ether. You
can still see the remnant of this in the names of the coefficients in Maxwell’s
equations.</p>
<h2 id="silly-models-yes-but-quantitative-and-predictive">Silly Models, yes. But quantitative, and Predictive.</h2>
<p>As silly as these assumptions may sound today, these models were quantitative and
they had predictive power. You could plug numbers into them and get numerical
predictions out. That’s tremendously useful to an engineer.</p>
<h2 id="seeking-a-modular-language-for-deep-learning-to-describe-the-action-of-each-layer">Seeking: A modular language for deep learning to describe the action of each layer.</h2>
<p>It would be easier to design deep nets if we could talk about the action of
each of its layers the way we talk about the action of an optical element
on the light that passes through it.</p>
<p>We talk about convolutional layers as running matched filters against their
inputs, and the subsequent nonlinearities as pooling. This is a relatively low-level
description, akin to describing the action of a lens in terms of Maxwell’s equations.</p>
<p>Maybe there are higher level abstractions to rely on, in terms of a
quantity that is modified as it passes through the layers of a net, akin to the action
of lens in terms of how it bends rays.</p>
<p>And it would be nice if this abstraction were quantitative so you could
plug numbers into a formula to run back-of-the-envelope analyses to help
you design your network.</p>
<h2 id="were-far-from-this-kind-of-language-lets-start-with-something-simpler">We’re far from this kind of language. Let’s start with something simpler.</h2>
<p>But I’m getting carried away with fantasies.</p>
<p>Let’s start with something simpler. We have lots of mental models of how
deep net training works. I’ve collected a sampling of phenomena worth explaining. Let’s
see how well these mental models explain these phenomena.</p>
<p>Before I get too far, I admit that this exercise is doomed. Optics had 300 years
to do this. I spent a Saturday afternoon on it. In exchange I’m just reporting
my findings in a blog.</p>
<h4 id="phenomenon-a-random-initializer-for-sgd-is-good-enough-but-small-numerical-errors-thereafter-or-bad-step-sizes-break-sgd"><em>Phenomenon:</em> A random initializer for SGD is good enough. But small numerical errors thereafter, or bad step sizes, break SGD.</h4>
<p>Some practitioners have observed that small changes in how gradients are
accumulated can result in vastly different performance on the test set. This
comes up for example, when you train with a GPU instead of a CPU
(<a href="https://github.com/tensorflow/tensorflow/issues/2226">1</a>,
<a href="https://github.com/tensorflow/tensorflow/issues/2732">2</a>).</p>
<p>Do you think this is a legitimate observation that’s worth explaining?</p>
<p>Or do you think this a spurious observation, and it’s probably untrue?</p>
<p>Or maybe you think there is something wrong with this observation, like it’s
logically self-contradictory in some way, or it’s a malformed claim?</p>
<p>Sentiments are mixed, I’m sure, but let me record this as a phenomenon and
move on.</p>
<h4 id="phenomenon-shallow-local-minima-generalize-better-than-sharp-ones"><em>Phenomenon:</em> Shallow local minima generalize better than sharp ones.</h4>
<p>This claim is fashionable. Some people insist it’s true
(<a href="https://arxiv.org/abs/1609.04836">3</a>, <a href="https://arxiv.org/abs/1611.01838">4</a>,
<a href="https://arxiv.org/abs/1704.04289">5</a>, <a href="https://arxiv.org/abs/1710.06451">6</a>), and others,
including myself, believe that it can’t be true on logical grounds.
Others have refuted the claim on empirical grounds
(<a href="https://arxiv.org/abs/1703.04933">7</a>). Yet others have offered refined variants
of this claim (<a href="https://arxiv.org/abs/1706.08947">8</a>). Confusion
remains (<a href="https://twitter.com/beenwrekt/status/941005520420225025">9</a>).</p>
<p>I’ll note that this phenomenon might be disputed, but record it nevertheless.</p>
<h4 id="phenomenon-inserting-batch-norm-layers-speeds-up-sgd"><em>Phenomenon:</em> Inserting Batch Norm layers speeds up SGD.</h4>
<p>That batchnorm works is mostly undisputed. I offer only one redoubtable
exception (<a href="http://nyus.joshuawise.com/batchnorm.pdf">10</a>)), and record this
as a phenomenon with no reservation.</p>
<h4 id="phenomenon-sgd-is-successful-despite-many-local-optima-and-saddle-points"><em>Phenomenon:</em> SGD is successful despite many local optima and saddle points.</h4>
<p>There are several claims packed in this one.  It is often claimed that the training
loss surface of deep learning is rife with saddle points and local
minima (<a href="https://arxiv.org/abs/1712.04741">11</a>). In addition, it is variously
claimed either that gradient descent can traverse these hazards
(<a href="https://arxiv.org/abs/1412.6544">12</a>), or that it need not traverse these hazards to
produce a solution that generalizes well
(<a href="https://arxiv.org/abs/1712.04741">13</a>).
It is also just as often claimed that the loss surface of deep models is altogether benign
(<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Haeffele_Global_Optimality_in_CVPR_2017_paper.html">14</a>).</p>
<p>I’ll record this a phenomenon, begrudgingly.</p>
<h4 id="phenomenon-dropout-works-better-than-other-randomization-strategies"><em>Phenomenon:</em> Dropout works better than other randomization strategies.</h4>
<p>I don’t know how to categorize the class of dropout-like things, so I’m calling
them “randomization strategies”.</p>
<p>Recorded with apologies and without discussion.</p>
<h4 id="phenomenon-deep-nets-can-memorize-random-labels-and-yet-they-generalize"><em>Phenomenon:</em> Deep nets can memorize random labels, and yet, they generalize.</h4>
<p>The evidence is plain (<a href="https://arxiv.org/abs/1611.03530">15</a>),
witnessed and supported by dear friends.</p>
<p>Recorded as a phenomenon despite the conflict of interest.</p>
<h2 id="explications">Explications</h2>
<p>We’ve gathered some phenomena. From the papers I cited above, I’ve also gathered
theories that I think have the best chance of explaining these phenomena.</p>
<p>Let’s see how we fare:</p>
<p class="center"><img alt="You're fired" src="/assets/optics/deeplearning-scorecard.png" width="650px"/></p>
<p>There are a few reasons to not get excited.</p>
<p>First, we couldn’t agree on some of the observations we were trying to
explain in the first place.</p>
<p>Second, I couldn’t organize the explanations into a hierarchy of
abstractions. What made optics easy to learn isn’t at all manifest here.</p>
<p>Third, I suspect some of the theories I’m quoting from the papers aren’t true.</p>
<h2 id="my-point">My Point</h2>
<p>There’s a mass influx of newcomers to our field and we’re equipping them with
little more than folklore and pre-trained deep nets, then asking them to innovate.
We can barely agree on the phenomena that we should be explaining away. I think
we’re far from teaching this stuff in high schools.</p>
<p>How do we get there?</p>
<p>It would be nice if we could provide mental models, at various layers of
abstraction, of the action of the layers of a deep net. What could be our
equivalent of refraction, dispersion, and diffraction? Maybe you already think
in terms of these actions, but we just haven’t standardized our language
around these concepts?</p>
<p>Let’s gather a set of phenomena that we all agree on. We could
then try to explain them away. What is our equivalent of Newton Rings, the
Kerr effect, or the Faraday effect?</p>
<p>A small group of colleagues and I have started a massive empirical effort to
catalogue mental models that are ambient in our field, to formalize them, and
to then validate them with experiments. It’s a lot of work. I think it’s the
first step to developing a layered mental model of deep learning that you
could teach in high schools. If you want to chat about this project or to help
out, please <a href="https://twitter.com/beenwrekt/status/956621165161779200">tweet me</a>.</p>
</article>